# reg_si.py (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)

import os
import numpy as np
import pandas as pd
import shap
import joblib
import matplotlib.pyplot as plt
import logging
from sklearn.base import clone

from catboost import CatBoostRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import SelectFromModel
from sklearn.impute import SimpleImputer
from sklearn.exceptions import ConvergenceWarning
import warnings

# === –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ===
TARGET = "log1p_SI"
TASK_NAME = "reg_si"
DATA_FILE = "data/eda_gen/data_final.csv"
PLOTS_DIR = f"plots/shap/{TASK_NAME}"
MODEL_DIR = os.path.join("models", "regression", TASK_NAME)
MODEL_OUT = os.path.join(MODEL_DIR, f"{TASK_NAME}_model.joblib")
FEATURES_OUT_JOBLIB = os.path.join(MODEL_DIR, f"{TASK_NAME}_features.joblib")
METRICS_OUT = f"data/metrics_{TASK_NAME}.csv"
PREDS_OUT = f"data/preds_{TASK_NAME}.csv"
os.makedirs(PLOTS_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

warnings.filterwarnings("ignore", category=ConvergenceWarning)

def main():
    np.random.seed(42)
    logger.info(f"--- SHAP-–∞–Ω–∞–ª–∏–∑ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ SI ---")

    df = pd.read_csv(DATA_FILE)
    if TARGET not in df.columns:
        logger.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ {TARGET} –æ—Ç—Å—É—Ç—Å—Ç—É–µ—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ")
        return

    y = df[TARGET]
    X_raw = df.drop(columns=[TARGET])
    
    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    X_imputed = pd.DataFrame(SimpleImputer(strategy="median").fit_transform(X_raw), columns=X_raw.columns)

    forbidden = [
        "CC50", "CC50_mM", "CC50_nM", "log_CC50", "log1p_CC50", "log1p_CC50_nM", "CC50_gt_median",
        "IC50", "IC50_mM", "IC50_nM", "log_IC50", "log1p_IC50", "log1p_IC50_nM", "IC50_gt_median",
        "SI", "SI_corrected", "log_SI", "log1p_SI", "log1p_SI_corrected",
        "SI_original", "SI_diff", "SI_diff_check", "SI_check", "SI_gt_median",  "SI_gt_8",
        "ratio_IC50_CC50", "Unnamed: 0"
    ]
    X = X_imputed.drop(columns=[col for col in forbidden if col in X_imputed.columns], errors="ignore")
    logger.info(f"–£–¥–∞–ª–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É—Ç–µ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.")

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # GridSearchCV –¥–ª—è CatBoost
    logger.info("üîß GridSearchCV –¥–ª—è CatBoost...")
    param_grid = {"iterations": [800], "depth": [6], "learning_rate": [0.03]}
    grid_model = GridSearchCV(
        estimator=CatBoostRegressor(loss_function="RMSE", eval_metric="R2", verbose=0, random_state=42),
        param_grid=param_grid, cv=5, scoring="r2", n_jobs=-1
    )
    grid_model.fit(X_train, y_train)
    best_cat_model = grid_model.best_estimator_
    logger.info(f"üîç CatBoost (—Ç—é–Ω–∏–Ω–≥): R¬≤ = {r2_score(y_test, best_cat_model.predict(X_test)):.4f}")

    # –û–±—É—á–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
    models = {
        "RandomForest": RandomForestRegressor(n_estimators=800, max_depth=10, random_state=42),
        "GradientBoosting": GradientBoostingRegressor(n_estimators=800, learning_rate=0.03, max_depth=6, random_state=42),
    }
    for name, model in models.items():
        model.fit(X_train, y_train)
        logger.info(f"üîç {name}: R¬≤ = {r2_score(y_test, model.predict(X_test)):.4f}")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (CatBoost vs –æ—Å—Ç–∞–ª—å–Ω—ã–µ)
    all_models = {"CatBoost": best_cat_model, **models}
    best_model_name = max(all_models, key=lambda name: r2_score(y_test, all_models[name].predict(X_test)))
    best_model = all_models[best_model_name]
    logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} (R¬≤ –Ω–∞ —Ç–µ—Å—Ç–µ = {r2_score(y_test, best_model.predict(X_test)):.4f})")

    # SHAP-–∞–Ω–∞–ª–∏–∑ –Ω–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    logger.info(f"üî¨ SHAP-–∞–Ω–∞–ª–∏–∑ –¥–ª—è –º–æ–¥–µ–ª–∏ {best_model_name}...")
    explainer = shap.TreeExplainer(best_model)
    shap_values = explainer.shap_values(X) # –ê–Ω–∞–ª–∏–∑ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è SHAP
    shap.summary_plot(shap_values, X, plot_type="bar", max_display=20, show=False)
    plt.tight_layout(); plt.savefig(f"{PLOTS_DIR}/shap_bar_{TARGET}_top20.png"); plt.close()

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ SHAP
    sfm = SelectFromModel(best_model, threshold="median", prefit=True)
    final_features = X.columns[sfm.get_support()].tolist()
    logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ SHAP/SelectFromModel: {len(final_features)}")
    
    # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    logger.info(f"‚öôÔ∏è –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {best_model_name} –Ω–∞ {len(final_features)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...")
    X_final = X[final_features]
    model_to_save = clone(best_model)
    model_to_save.fit(X_final, y)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    joblib.dump(model_to_save, MODEL_OUT)
    logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_OUT}")

    joblib.dump(final_features, FEATURES_OUT_JOBLIB)
    logger.info(f"üìã –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {FEATURES_OUT_JOBLIB}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –æ—Ç—á–µ—Ç–∞
    metrics = {name: r2_score(y_test, model.predict(X_test)) for name, model in all_models.items()}
    pd.DataFrame(metrics.items(), columns=["Model", "R2_on_test"]).to_csv(METRICS_OUT, index=False)
    logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {METRICS_OUT}")

    pd.DataFrame({"true": y_test, "pred_best_model": best_model.predict(X_test)}).to_csv(PREDS_OUT, index=False)
    logger.info(f"üìÇ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {PREDS_OUT}")

if __name__ == "__main__":
    main()