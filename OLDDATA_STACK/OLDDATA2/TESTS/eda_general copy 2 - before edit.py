# === eda_general.py ===
print("=== eda_general.py ===")

# === –≠–¢–ê–ü 0: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –±–∞–∑–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ===
print("=== –≠–¢–ê–ü 0: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –±–∞–∑–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ===")

# === –ò–ú–ü–û–†–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò ===

import matplotlib.gridspec as gridspec

# üì¶ –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import joblib

# üß™ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import scipy.stats as stats
from matplotlib import gridspec

# üß† ML / Feature Selection / –ú–µ—Ç—Ä–∏–∫–∏
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_regression, mutual_info_classif
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay
)
from sklearn.dummy import DummyRegressor, DummyClassifier
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# üåê –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ü–∏–π
import umap.umap_ as umap

# üõ†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤
warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", None)
pd.set_option("display.float_format", '{:.4f}'.format)
sns.set(style="whitegrid")

def main():

    # === –°–û–ó–î–ê–ù–ò–ï –ö–ê–¢–ê–õ–û–ì–û–í ===
    os.makedirs("data/eda_gen", exist_ok=True)
    os.makedirs("plots/eda_gen", exist_ok=True)
    os.makedirs("data/eda_gen/scaled", exist_ok=True)
    os.makedirs("data/eda_gen/features", exist_ok=True)
    os.makedirs("plots/eda_gen/projections_task/pca", exist_ok=True)
    os.makedirs("plots/eda_gen/projections_task/umap", exist_ok=True)
    os.makedirs("plots/eda_gen/projections_task/lda", exist_ok=True)
    os.makedirs("plots/eda_gen/projections_variance", exist_ok=True)
    os.makedirs("plots/eda_gen/targets/log_transform", exist_ok=True)
    os.makedirs("plots/eda_gen/targets/strip", exist_ok=True)
    os.makedirs("plots/eda_gen/classification_targets", exist_ok=True)
    os.makedirs("plots/eda_gen/classification_targets/analysis", exist_ok=True)
    os.makedirs("plots/eda_gen/classification_targets/dummy_reports", exist_ok=True)
    os.makedirs("plots/eda_gen/outliers", exist_ok=True)
    os.makedirs("plots/dummy", exist_ok=True)
    os.makedirs("plots/eda_gen/features", exist_ok=True)
    os.makedirs("plots/eda_gen/feature_importance", exist_ok=True)


    # üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ Excel-—Ñ–∞–π–ª–∞
    df = pd.read_excel("data/–î–∞–Ω–Ω—ã–µ_–¥–ª—è_–∫—É—Ä—Å–æ–≤–æ–∏_–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ_–ú–û.xlsx")

    # ‚úÖ –ü—É—Ç–∏ –∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –æ–±—ä–µ–∫—Ç–∞–º
    DATA_PATH = "data/eda_gen/data_final.csv"
    X_SCALED_PATH = "data/eda_gen/scaled/X_scaled.csv"
    SCALER_PATH = "data/eda_gen/scaled/scaler_clf.pkl"

    # üéØ –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    TARGET_COLUMNS = [
        "IC50_gt_median", "CC50_gt_median",
        "SI_gt_median", "SI_gt_8"
    ]

    # üö´ –ü—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ —Ñ–∏—á–∏
    FORBIDDEN_COLUMNS = [
        "IC50_nM", "CC50_nM", "SI_corrected",
        "log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI"
    ] + TARGET_COLUMNS

    # üñ®Ô∏è –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    print("‚úÖ –≠–¢–ê–ü 0: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
    print("üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å:", df.shape)
    print("üìã –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:")
    print(df.head(3))
    print("üì¶ –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(df.dtypes.value_counts())
    print("üîç Object-–∫–æ–ª–æ–Ω–∫–∏:")
    print(df.select_dtypes(include='object').columns.tolist())
    print("üìå –î—É–±–ª–∏–∫–∞—Ç—ã:", df.duplicated().sum())
    print("üìå –°—Ç—Ä–æ–∫ —Å >20 NaN:", (df.isnull().sum(axis=1) > 20).sum())

    # üìä –ü–æ–¥—Å—á—ë—Ç NaN –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º
    nan_stats = df.isnull().sum()
    nan_stats = nan_stats[nan_stats > 0].sort_values(ascending=False)
    if nan_stats.empty:
        print("‚úÖ –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç")
    else:
        print("‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏:")
        print(nan_stats)

    print("‚úÖ –≠–¢–ê–ü 0 –∑–∞–≤–µ—Ä—à—ë–Ω: –ò–º–ø–æ—Ä—Ç—ã –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω—ã")


    # === –≠–¢–ê–ü 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===


    # === –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ ===
    df = df.drop(columns=["Unnamed: 0"], errors="ignore")
    df.columns = df.columns.str.strip().str.replace(",", "").str.replace(" ", "_")

    # === –®–∞–≥ 2: –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ===
    df = df.drop_duplicates()
    df = df.fillna(df.mean(numeric_only=True))

    # === –®–∞–≥ 3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –Ω–∞–Ω–æ–ú–æ–ª–∏ ===
    df["IC50_nM"] = df["IC50_mM"] * 1e6
    df["CC50_nM"] = df["CC50_mM"] * 1e6

    # === –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ ===
    df["log1p_IC50_nM"] = np.log1p(df["IC50_nM"])
    df["log1p_CC50_nM"] = np.log1p(df["CC50_nM"])

    # === –†–∞—Å—á—ë—Ç SI –∏ –ª–æ–≥–∞—Ä–∏—Ñ–º ===
    df["SI_corrected"] = df["CC50_nM"] / df["IC50_nM"]
    df["SI_corrected"] = df["SI_corrected"].replace([np.inf, -np.inf], np.nan)
    df["log1p_SI"] = np.log1p(df["SI_corrected"])

    # === –£–¥–∞–ª–µ–Ω–∏–µ NaN –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–∞—Ö ===
    df = df.dropna(subset=["log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI"])

    # === –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö SI/IC50/CC50 –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ===
    df = df.drop(columns=["IC50_mM", "CC50_mM", "SI"], errors="ignore")

    # === –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–∞—Ä–≥–µ—Ç–æ–≤ ===
    df["IC50_gt_median"] = (df["IC50_nM"] > df["IC50_nM"].median()).astype(int)
    df["CC50_gt_median"] = (df["CC50_nM"] > df["CC50_nM"].median()).astype(int)
    df["SI_gt_median"] = (df["SI_corrected"] > df["SI_corrected"].median()).astype(int)
    df["SI_gt_8"] = (df["SI_corrected"] > 8).astype(int)


    # === –ò—Å—Ö–æ–¥–Ω—ã–µ –∏ –ª–æ–≥-–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ===
    targets = {
        "IC50": ("IC50_nM", "log1p_IC50_nM"),
        "CC50": ("CC50_nM", "log1p_CC50_nM"),
        "SI": ("SI_corrected", "log1p_SI")
    }

    # === –§—É–Ω–∫—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ/–ø–æ—Å–ª–µ ===
    def plot_comparison(before, after, label, save_name):
        fig = plt.figure(figsize=(16, 5))
        spec = gridspec.GridSpec(ncols=4, nrows=1, figure=fig)

        ax0 = fig.add_subplot(spec[0, 0])
        sns.histplot(before, kde=True, bins=30, ax=ax0, color="skyblue")
        ax0.set_title(f"{label} - –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ + KDE")

        ax1 = fig.add_subplot(spec[0, 1])
        sns.boxplot(y=before, ax=ax1, color="lightgreen")
        ax1.set_title(f"{label} - Boxplot")

        ax2 = fig.add_subplot(spec[0, 2])
        stats.probplot(before, dist="norm", plot=ax2)
        ax2.set_title("QQ Plot")

        mean = before.mean()
        std = before.std()
        skew = before.skew()
        kurt = before.kurt()
        iqr = before.quantile(0.75) - before.quantile(0.25)
        val_range = (before.min(), before.max())

        stats_text = f"""
        Mean      = {mean:.3f}
        Std       = {std:.3f}
        Skewness  = {skew:.3f}
        Kurtosis  = {kurt:.3f}
        IQR       = {iqr:.3f}
        Min-Max   = {val_range[0]:.3f} ‚Üí {val_range[1]:.3f}
        """

        ax3 = fig.add_subplot(spec[0, 3])
        ax3.text(0.1, 0.5, stats_text, fontsize=12)
        ax3.axis("off")
        ax3.set_title("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")

        plt.suptitle(f"{label}", fontsize=14)
        plt.tight_layout()
        plt.subplots_adjust(top=0.85)
        plt.savefig(f"plots/eda_gen/targets/log_transform/{save_name}.png")
        plt.close()

    # === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–æ/–ø–æ—Å–ª–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è ===
    for name, (raw_col, log_col) in targets.items():
        plot_comparison(df[raw_col], df[raw_col], f"{name} ‚Äî –î–û –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è", f"{name}_before_log")
        plot_comparison(df[log_col], df[log_col], f"{name} ‚Äî –ü–û–°–õ–ï –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è", f"{name}_after_log")

    # === –£–¥–∞–ª–µ–Ω–∏–µ —É—Ç–µ—á–µ–∫ ===
    df = df.drop(columns=["IC50_nM", "CC50_nM", "SI_corrected"], errors="ignore")

    # === –ü—Ä–æ–≤–µ—Ä–∫–∏ ===
    print("‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:", df.shape)
    print("\nüìä –û–ø–∏—Å–∞–Ω–∏–µ –ª–æ–≥-—Ç–∞—Ä–≥–µ—Ç–æ–≤:")
    print(df[["log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI"]].describe())

    print("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for col in ["IC50_gt_median", "CC50_gt_median", "SI_gt_median", "SI_gt_8"]:
        print(f"{col}:")
        print(df[col].value_counts(normalize=True).rename_axis("class").reset_index(name="fraction"))
        print()

    # === –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç ===
    df.to_csv("data/eda_gen/data_clean.csv", index=False)

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ===
    nan_counts = df.isnull().sum()
    nan_counts = nan_counts[nan_counts > 0]

    if nan_counts.empty:
        print("‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –ù–ï–¢.")
    else:
        print("üß® –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏:")
        print(nan_counts.sort_values(ascending=False))

    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: data/eda_gen/data_clean.csv")


    # === –≠–¢–ê–ü 2: –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ª–æ–≥-—Ç–∞—Ä–≥–µ—Ç–æ–≤ + —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ ===
    print("=== –≠–¢–ê–ü 2: –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ª–æ–≥-—Ç–∞—Ä–≥–µ—Ç–æ–≤ + —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ ===")



    # === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
    df = pd.read_csv("data/eda_gen/data_clean.csv")
    target_cols = ["log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI"]

    # === –§—É–Ω–∫—Ü–∏–∏ ===
    def get_stats_text(series):
        return f"""
        Mean      = {series.mean():.3f}
        Std       = {series.std():.3f}
        Skewness  = {series.skew():.3f}
        Kurtosis  = {series.kurt():.3f}
        IQR       = {series.quantile(0.75) - series.quantile(0.25):.3f}
        Min-Max   = {series.min():.3f} ‚Üí {series.max():.3f}
        """

    def get_quantiles_robust(series):
        skew = series.skew()
        kurt = series.kurt()
        n = len(series)

        if n < 200:
            method = "IQR"
            q1, q3 = series.quantile(0.25), series.quantile(0.75)
            return q1, q3, 1.5, method

        if abs(skew) > 2 or kurt > 5:
            return None, None, 3, "MAD"  # –ü—Ä–æ–ø—É—Å—Ç–∏–º MAD

        if abs(skew) > 1.2 or kurt > 2.5:
            method = "IQR"
            q1, q3 = series.quantile(0.20), series.quantile(0.60)
            return q1, q3, 1.0, method

        if abs(skew) > 0.5 or kurt > 1.5:
            method = "IQR"
            q1, q3 = series.quantile(0.22), series.quantile(0.68)
            return q1, q3, 1.5, method

        method = "IQR"
        q1, q3 = series.quantile(0.25), series.quantile(0.75)
        return q1, q3, 1.5, method

    def plot_ecdf(series, title, path):
        x = np.sort(series)
        y = np.arange(1, len(x) + 1) / len(x)
        plt.figure(figsize=(5, 4))
        plt.plot(x, y, marker='.', linestyle='none', color='blue')
        plt.xlabel(title)
        plt.ylabel("–î–æ–ª—è ‚â§ –∑–Ω–∞—á–µ–Ω–∏—è")
        plt.title(f"ECDF: {title}")
        plt.grid(True)
        plt.savefig(path)
        plt.close()

    # === 2.1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ ===
    for col in target_cols:
        print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {col}")

        fig = plt.figure(figsize=(24, 5))
        spec = gridspec.GridSpec(ncols=4, nrows=1, figure=fig)

        ax0 = fig.add_subplot(spec[0, 0])
        sns.histplot(df[col], kde=True, bins=30, color="skyblue", ax=ax0)
        ax0.set_title(f"{col} ‚Äî Histogram (Before)")

        ax1 = fig.add_subplot(spec[0, 1])
        sns.boxplot(y=df[col], color="salmon", ax=ax1)
        ax1.set_title(f"{col} ‚Äî Boxplot (Before)")

        ax2 = fig.add_subplot(spec[0, 2])
        stats.probplot(df[col], dist="norm", plot=ax2)
        ax2.set_title(f"{col} ‚Äî QQ Plot (Before)")

        ax3 = fig.add_subplot(spec[0, 3])
        ax3.text(0.05, 0.5, get_stats_text(df[col]), fontsize=12, verticalalignment='center')
        ax3.axis("off")
        ax3.set_title("Stats (Before)")

        plt.tight_layout()
        plt.savefig(f"plots/eda_gen/outliers/{col}_before_outliers.png")
        plt.close()

        plot_ecdf(df[col], f"{col} (Before)", f"plots/eda_gen/outliers/{col}_ecdf_before.png")

        Q1, Q3, multiplier, method = get_quantiles_robust(df[col])

        if method == "MAD":
            print(f"‚ö†Ô∏è  {col}: –º–µ—Ç–æ–¥ MAD –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        IQR = Q3 - Q1
        lower = Q1 - multiplier * IQR
        upper = Q3 + multiplier * IQR

        outliers = df[(df[col] < lower) | (df[col] > upper)]
        print(f"üß® –£–¥–∞–ª—è–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ {col}: {outliers.shape[0]} —Å—Ç—Ä–æ–∫ (–º–µ—Ç–æ–¥: {method}, –º–Ω–æ–∂–∏—Ç–µ–ª—å={multiplier})")
        print(f"  ‚Üí –ö–≤–∞–Ω—Ç–∏–ª–∏: Q1={Q1:.2f}, Q3={Q3:.2f}")
        outliers.to_csv(f"plots/eda_gen/outliers/removed_{col}.csv", index=False)

        df = df[(df[col] >= lower) & (df[col] <= upper)]

        fig = plt.figure(figsize=(24, 5))
        spec = gridspec.GridSpec(ncols=4, nrows=1, figure=fig)

        ax0 = fig.add_subplot(spec[0, 0])
        sns.histplot(df[col], kde=True, bins=30, color="lightgreen", ax=ax0)
        ax0.set_title(f"{col} ‚Äî Histogram (After)")

        ax1 = fig.add_subplot(spec[0, 1])
        sns.boxplot(y=df[col], color="lightblue", ax=ax1)
        ax1.set_title(f"{col} ‚Äî Boxplot (After)")

        ax2 = fig.add_subplot(spec[0, 2])
        stats.probplot(df[col], dist="norm", plot=ax2)
        ax2.set_title(f"{col} ‚Äî QQ Plot (After)")

        ax3 = fig.add_subplot(spec[0, 3])
        ax3.text(0.05, 0.5, get_stats_text(df[col]), fontsize=12, verticalalignment='center')
        ax3.axis("off")
        ax3.set_title("Stats (After)")

        plt.tight_layout()
        plt.savefig(f"plots/eda_gen/outliers/{col}_after_outliers.png")
        plt.close()

        plot_ecdf(df[col], f"{col} (After)", f"plots/eda_gen/outliers/{col}_ecdf_after.png")

    # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    df.to_csv("data/eda_gen/data_clean_after_outliers.csv", index=False)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {df.shape[0]} —Å—Ç—Ä–æ–∫")

    # === 2.2: Dummy Regressor ===
    forbidden_cols = [
        "IC50", "IC50_mM", "IC50_nM", "log1p_IC50", "log_IC50", "log1p_IC50_nM", "IC50_gt_median",
        "CC50", "CC50_mM", "CC50_nM", "log1p_CC50", "log_CC50", "log1p_CC50_nM", "CC50_gt_median",
        "SI", "SI_corrected", "log_SI", "log1p_SI", "SI_gt_median", "SI_gt_8",
        "SI_original", "SI_diff", "SI_diff_check", "SI_check",
        "ratio_IC50_CC50", "Unnamed: 0"
    ]

    for target in target_cols:
        print(f"\nüìä Dummy Regressor Report ‚Äî {target}")
        y = df[target]
        X = df.drop(columns=forbidden_cols, errors="ignore").select_dtypes(include="number")

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = DummyRegressor(strategy="mean")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"RMSE: {rmse:.4f}")
        print(f"MAE:  {mae:.4f}")
        print(f"R¬≤:   {r2:.4f} [baseline: 0.0000]")


        residuals = y_test - y_pred

        # === üìà –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫: scatter + residual ===
        fig, axs = plt.subplots(1, 2, figsize=(12, 5))

        axs[0].scatter(y_test, y_pred, alpha=0.6, edgecolors="k")
        axs[0].plot([y.min(), y.max()], [y.min(), y.max()], linestyle="--", color="red")
        axs[0].set_title(f"Dummy: {target}")
        axs[0].set_xlabel("True Values")
        axs[0].set_ylabel("Predicted")
        axs[0].grid(True, linestyle="--", alpha=0.3)

        axs[1].hist(residuals, bins=30, color="skyblue", edgecolor="black")
        axs[1].axvline(0, color="red", linestyle="--")
        axs[1].set_title("Residuals")
        axs[1].set_xlabel("Prediction Error")
        axs[1].set_ylabel("Count")
        axs[1].grid(True, linestyle="--", alpha=0.3)

        fig.suptitle(f"Dummy Regressor ‚Äî {target}", fontsize=14)
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.savefig(f"plots/dummy/{target}_combined.png")
        plt.close()


    # === 2.3: Stripplot –ø–æ –±–∏–Ω–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–∞–º ===
    binary_targets = ["IC50_gt_median", "CC50_gt_median", "SI_gt_median", "SI_gt_8"]
    plots_per_figure = 4  # 2x2

    for i, hue_target in enumerate(binary_targets):
        fig, axs = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle(f"Stripplots by {hue_target}", fontsize=14)

        for j, reg_col in enumerate(target_cols):
            row, col = divmod(j, 2)
            ax = axs[row, col]

            sns.stripplot(
                x=df[hue_target].astype(str),
                y=df[reg_col],
                jitter=True,
                alpha=0.5,
                palette="Set2",
                edgecolor="gray",
                linewidth=0.3,
                ax=ax
            )
            ax.set_title(f"{reg_col} –ø–æ –∫–ª–∞—Å—Å–∞–º {hue_target}")
            ax.set_xlabel(hue_target)
            ax.set_ylabel(reg_col)
            ax.grid(True, linestyle="--", alpha=0.3)

            if j == plots_per_figure - 1 or j == len(target_cols) - 1:
                plt.tight_layout(rect=[0, 0, 1, 0.95])
                plt.savefig(f"plots/eda_gen/targets/strip/group_{hue_target}.png")
                plt.close()
                break  # –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –≥—Ä—É–ø–ø–µ


    # === –≠–¢–ê–ü 3: –ê–Ω–∞–ª–∏–∑ –±–∏–Ω–∞—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ ===
    print("\n=== –≠–¢–ê–ü 3: –ê–Ω–∞–ª–∏–∑ –±–∏–Ω–∞—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ ===")

    # === –ë–∏–Ω–∞—Ä–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã ===
    binary_targets = ["IC50_gt_median", "CC50_gt_median", "SI_gt_median", "SI_gt_8"]

    # === –≠–¢–ê–ü 3.1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ barplot'–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º —Ö–æ–ª—Å—Ç–µ ===
    fig, axes = plt.subplots(2, 2, figsize=(10, 8))
    axes = axes.flatten()

    for i, col in enumerate(binary_targets):
        counts = df[col].value_counts().sort_index()
        percentages = counts / counts.sum() * 100

        sns.barplot(x=counts.index.astype(str), y=counts.values, palette="Set2", ax=axes[i])
        for j, (v, p) in enumerate(zip(counts.values, percentages.values)):
            axes[i].text(j, v + 2, f"{v} ({p:.1f}%)", ha="center", fontsize=9)
        axes[i].set_title(f"{col} ‚Äî –∫–ª–∞—Å—Å—ã")
        axes[i].set_xlabel("–ö–ª–∞—Å—Å")
        axes[i].set_ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")

    plt.suptitle("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤", fontsize=14)
    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    plt.savefig("plots/eda_gen/classification_targets/all_distributions.png")
    plt.close()

    # –¢–∞–∫–∂–µ –ª–æ–≥ –≤ –∫–æ–Ω—Å–æ–ª—å
    for col in binary_targets:
        counts = df[col].value_counts()
        percentages = counts / counts.sum() * 100
        print(f"üìä {col}:")
        for cls in counts.index:
            print(f"  –ö–ª–∞—Å—Å {cls}: {counts[cls]} ({percentages[cls]:.2f}%)")
        print("-" * 50)

    # === –≠–¢–ê–ü 3.2: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ ===
    plt.figure(figsize=(6, 5))
    corr = df[binary_targets].corr()
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", square=True)
    plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏")
    plt.tight_layout()
    plt.savefig("plots/eda_gen/classification_targets/analysis/binary_targets_corr_heatmap.png")
    plt.close()

    # === –≠–¢–ê–ü 3.3: Violinplot –ø–æ MolLogP / SI_gt_8 ===
    if "MolLogP" in df.columns:
        plt.figure(figsize=(6, 4))
        sns.violinplot(x="SI_gt_8", y="MolLogP", data=df, palette="Set2")
        plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MolLogP –ø–æ –∫–ª–∞—Å—Å–∞–º SI_gt_8")
        plt.tight_layout()
        plt.savefig("plots/eda_gen/classification_targets/analysis/MolLogP_by_SI_gt_8.png")
        plt.close()

    # === –≠–¢–ê–ü 3.4: DummyClassifier sanity-check ===
    for target in binary_targets:
        print(f"\nüìå DummyClassifier: {target}")

        # === –£–¥–∞–ª–µ–Ω–∏–µ —É—Ç–µ—á–µ–∫ ===
        forbidden_cols = binary_targets + [
            # CC50-related
            "CC50", "CC50_mM", "CC50_nM", "log_CC50", "log1p_CC50", "log1p_CC50_nM", "CC50_gt_median",

            # IC50-related
            "IC50", "IC50_mM", "IC50_nM", "log_IC50", "log1p_IC50", "log1p_IC50_nM", "IC50_gt_median",

            # SI-related
            "SI", "SI_corrected", "log_SI", "log1p_SI", "log1p_SI_corrected",
            "SI_original", "SI_diff", "SI_diff_check", "SI_check", "SI_gt_median",	"SI_gt_8",

            # Other leakage-related
            "ratio_IC50_CC50", "Unnamed: 0"
        ]
        X = df.drop(columns=forbidden_cols, errors="ignore").select_dtypes(include="number")
        y = df[target]

        stratify_y = y if y.nunique() >= 2 and y.value_counts().min() >= 2 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=stratify_y
        )

        model = DummyClassifier(strategy="most_frequent")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # === –û—Ç—á–µ—Ç ===
        print(classification_report(y_test, y_pred, zero_division=0))

        # === –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ ===
        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["0", "1"])
        disp.plot(cmap="Blues", values_format="d")
        plt.title(f"DummyClassifier ‚Äî {target}")
        plt.tight_layout()
        plt.savefig(f"plots/eda_gen/classification_targets/dummy_reports/{target}_conf_matrix.png")
        plt.close()



    # === –≠–¢–ê–ü 4: Feature Engineering –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===

    print("\n=== –≠–¢–ê–ü 4: Feature Engineering –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")


    # === –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–Ω–µ –ø—Ä–∏–∑–Ω–∞–∫–∏) ===
    target_cols = [
        "log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI",
        "IC50_gt_median", "CC50_gt_median", "SI_gt_median", "SI_gt_8"
    ]

    # === Feature Engineering ===
    print("\nüìå –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    if "MaxEStateIndex" in df.columns and "MinEStateIndex" in df.columns:
        df["EState_Delta"] = df["MaxEStateIndex"] - df["MinEStateIndex"]
        print("‚úÖ EState_Delta")
    if "NumHAcceptors" in df.columns and "NumHDonors" in df.columns:
        df["HAcceptors_to_HDonors_Ratio"] = df["NumHAcceptors"] / (df["NumHDonors"] + 1e-6)
        print("‚úÖ HAcceptors_to_HDonors_Ratio")
    if "MolLogP" in df.columns:
        df["MolLogP_sq"] = df["MolLogP"] ** 2
        print("‚úÖ MolLogP_sq")
    if "MolWt" in df.columns and "TPSA" in df.columns:
        df["MolWt_x_TPSA"] = df["MolWt"] * df["TPSA"]
        print("‚úÖ MolWt_x_TPSA")

    # === –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–±–æ—Ä —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    X = df.select_dtypes(include=[np.number]).drop(columns=target_cols, errors="ignore")
    print(f"\nüî¢ –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {X.shape[1]}")

    # === –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å >30% NaN ===
    nan_ratio = X.isna().mean()
    nan_features = nan_ratio[nan_ratio > 0.2].index.tolist()
    print(f"‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å >20% NaN: {len(nan_features)}")

    # === –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ===
    constant_features = X.columns[X.nunique(dropna=False) <= 1].tolist()
    print(f"‚ùå –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(constant_features)}")
    print(constant_features)


    # === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_std = X.std()

    plt.figure(figsize=(10, 4))
    sns.histplot(X_std, bins=50, kde=True)
    plt.axvline(0.01, color='red', linestyle='--', label='–ü–æ—Ä–æ–≥ 0.01')
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    plt.xlabel("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ")
    plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    plt.legend()
    plt.tight_layout()
    plt.savefig("plots/eda_gen/features/std_threshold_exploration.png")
    plt.close()


    # === –ù–∏–∑–∫–æ–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (std < 0.01) ===
    low_variance_features = X.columns[X.std() < 0.01].tolist()
    print(f"‚ö†Ô∏è –ù–∏–∑–∫–æ–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (< 0.01): {len(low_variance_features)}")
    
    low_var_feats = X.std()[X.std() < 0.01].sort_values()
    print(low_var_feats)


    # === –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫ —É–¥–∞–ª–µ–Ω–∏—é ===
    bad_features = sorted(set(nan_features + constant_features + low_variance_features))
    print(f"üßπ –£–¥–∞–ª—è–µ—Ç—Å—è –≤—Å–µ–≥–æ: {len(bad_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    with open("data/eda_gen/features/features_to_remove_preliminary.txt", "w") as f:
        for feat in bad_features:
            f.write(f"{feat}\n")

    # === –£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ ===
    df = df.drop(columns=bad_features, errors="ignore")
    print(f"‚úÖ –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è: {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")

    # === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è std —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π ===
    plt.figure(figsize=(10, 4))
    X_std = df.select_dtypes(include=[np.number]).drop(columns=target_cols, errors="ignore").std()
    sns.histplot(X_std, bins=30, kde=True)
    plt.axvline(0.01, color='red', linestyle='--', label='–ü–æ—Ä–æ–≥ 0.01')
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    plt.xlabel("Std")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.legend()
    plt.tight_layout()
    plt.savefig("plots/eda_gen/features/std_distribution.png")
    plt.close()



    # === –ò–º–ø—É—Ç–∞—Ü–∏—è NaN —Å—Ä–µ–¥–Ω–∏–º ===
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if df[num_cols].isnull().any().any():
        imputer = SimpleImputer(strategy="median")
        df[num_cols] = imputer.fit_transform(df[num_cols])
        print("‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –º–µ–¥–∏–∞–Ω–æ–π.")
    else:
        print("‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç.")

    # === –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ===
    df.to_csv("data/eda_gen/data_clean_pruned.csv", index=False)
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: data/eda_gen/data_clean_pruned.csv")

    # === –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    remaining_features = df.drop(columns=target_cols, errors='ignore').select_dtypes(include='number').columns.tolist()
    print(f"üîé –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(remaining_features)}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüßæ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    with open("data/eda_gen/features/features_to_remove_preliminary.txt") as f:
        bad_features = [line.strip() for line in f]
    print(f"–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(bad_features)}")
    for col in bad_features[:20]:
        print(f"  ‚Ä¢ {col}")






    # === –≠–¢–ê–ü 5: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ MI —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π ===
    print("\n=== –≠–¢–ê–ü 5: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ MI —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π ===")



    # === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
    df = pd.read_csv("data/eda_gen/data_clean_pruned.csv")

    # === –°–ª–æ–≤–∞—Ä–∏ –∑–∞–¥–∞—á ===
    tasks = {
        "reg_log1p_IC50_nM": df["log1p_IC50_nM"],
        "reg_log1p_CC50_nM": df["log1p_CC50_nM"],
        "reg_log1p_SI": df["log1p_SI"],
        "clf_IC50_gt_median": df["IC50_gt_median"],
        "clf_CC50_gt_median": df["CC50_gt_median"],
        "clf_SI_gt_median": df["SI_gt_median"],
        "clf_SI_gt_8": df["SI_gt_8"]
    }

    forbidden_cols = [
        # CC50-related
        "CC50", "CC50_mM", "CC50_nM", "log_CC50", "log1p_CC50", "log1p_CC50_nM", "CC50_gt_median",

        # IC50-related
        "IC50", "IC50_mM", "IC50_nM", "log_IC50", "log1p_IC50", "log1p_IC50_nM", "IC50_gt_median",

        # SI-related
        "SI", "SI_corrected", "log_SI", "log1p_SI", "log1p_SI_corrected",
        "SI_original", "SI_diff", "SI_diff_check", "SI_check", "SI_gt_median",	"SI_gt_8",

        # Other leakage-related
        "ratio_IC50_CC50", "Unnamed: 0"
    ]

    # === –ü—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–æ–≤ ===
    X = df.drop(columns=forbidden_cols, errors="ignore")
    print(f"‚úÖ –ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è MI: {X.shape[1]}")

    # === –†–∞—Å—á—ë—Ç MI –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º ===
    # === –†–∞—Å—á—ë—Ç MI –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ===
    mi_all = {}
    mi_summary = {}

    fig_bar, axs_bar = plt.subplots(3, 3, figsize=(18, 14))
    fig_cum, axs_cum = plt.subplots(3, 3, figsize=(18, 14))
    axs_bar = axs_bar.flatten()
    axs_cum = axs_cum.flatten()

    for idx, (task_name, y) in enumerate(tasks.items()):
        is_clf = task_name.startswith("clf_")
        mi = mutual_info_classif(X, y, random_state=42) if is_clf else mutual_info_regression(X, y, random_state=42)
        mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)
        mi_all[task_name] = mi_series

        cumulative_mi = mi_series.cumsum() / mi_series.sum()
        optimal_k = (cumulative_mi < 0.95).sum() + 1
        top_features = mi_series.head(optimal_k)
        mi_summary[task_name] = optimal_k

        top_features.to_csv(f"data/eda_gen/features/topMI_{task_name}.csv", header=["mutual_info"])
        with open(f"data/eda_gen/features/{task_name}.txt", "w") as f:
            f.writelines([f"{feat}\n" for feat in top_features.index])

        # === Barplot ===
        ax_bar = axs_bar[idx]
        sns.barplot(x=top_features.head(30).values, y=top_features.head(30).index, ax=ax_bar, palette="viridis")
        ax_bar.set_title(f"Top-30 MI: {task_name}", fontsize=10)
        ax_bar.set_xlabel("Mutual Information")
        ax_bar.set_ylabel("")

        # === –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ ===
        ax_cum = axs_cum[idx]
        ax_cum.plot(range(1, len(cumulative_mi)+1), cumulative_mi.values, marker='o')
        ax_cum.axhline(0.95, color='r', linestyle='--', label='95% –ø–æ—Ä–æ–≥')
        ax_cum.axvline(optimal_k, color='g', linestyle='--', label=f"K = {optimal_k}")
        ax_cum.set_title(f"–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è MI: {task_name}", fontsize=10)
        ax_cum.set_xlabel("–ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        ax_cum.set_ylabel("–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–æ–ª—è MI")
        ax_cum.grid(True)
        ax_cum.legend(fontsize=8)

        print(f"üìå {task_name}: –≤—ã–±—Ä–∞–Ω–æ {optimal_k} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. ‚úÖ")

    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –æ—Å–∏
    for j in range(len(tasks), len(axs_bar)):
        fig_bar.delaxes(axs_bar[j])
        fig_cum.delaxes(axs_cum[j])

    # === –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏ ===
    fig_bar.tight_layout()
    fig_bar.savefig("plots/eda_gen/feature_importance/all_barplots_MI.png")
    plt.close()

    fig_cum.tight_layout()
    fig_cum.savefig("plots/eda_gen/feature_importance/all_cumulative_MI.png")
    plt.close()




    # === üìä –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø–æ—Ä–æ–≥–∞ MI –Ω–∞ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    thresholds = [0.90, 0.95, 0.98, 0.99]
    mi_threshold_summary = pd.DataFrame(index=tasks.keys(), columns=thresholds)

    for task_name, mi_series in mi_all.items():
        cumsum = mi_series.cumsum() / mi_series.sum()
        for thresh in thresholds:
            k = (cumsum < thresh).sum() + 1
            mi_threshold_summary.loc[task_name, thresh] = k

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –≤—ã–≤–æ–¥–∏–º
    mi_threshold_summary = mi_threshold_summary.astype(int)
    print("\n=== üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–∞—Ö MI ===")
    print(mi_threshold_summary)

    mi_threshold_summary.to_csv("data/eda_gen/features/mi_k_by_threshold.csv")


    # === üìà –ì—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø–æ—Ä–æ–≥–∞ MI –æ—Ç —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    plt.figure(figsize=(10, 6))
    for task_name in mi_threshold_summary.index:
        plt.plot(thresholds, mi_threshold_summary.loc[task_name], marker='o', label=task_name)

    plt.title("–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç –ø–æ—Ä–æ–≥–∞ MI")
    plt.xlabel("–ü–æ—Ä–æ–≥ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π MI")
    plt.ylabel("–ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig("plots/eda_gen/feature_importance/mi_k_vs_threshold.png")
    plt.close()








    # === –°–≤–æ–¥–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    print("\n=== –°–≤–æ–¥–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
    for task_name, k in mi_summary.items():
        print(f"{task_name}: {k} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    # === –¢–û–ü-30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ MI ===
    print("\n=== –¢–û–ü-30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ MI –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ ===")
    for task_name, mi_series in mi_all.items():
        print(f"\nüìå {task_name} ‚Äî top 30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, (feat, score) in enumerate(mi_series.head(30).items(), 1):
            print(f"{i:2d}. {feat:<30} ‚Üí MI = {score:.4f}")

    # === –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ MI_avg ===
    mi_df = pd.DataFrame(mi_all)
    for col in forbidden_cols:
        assert col not in mi_df.index, f"üö® –£—Ç–µ—á–∫–∞! –ü—Ä–∏–∑–Ω–∞–∫ {col} –ø–æ–ø–∞–ª –≤ –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É MI."

    mi_df["MI_avg"] = mi_df.mean(axis=1)
    mi_ranked = mi_df.sort_values("MI_avg", ascending=False)
    mi_ranked.to_csv("data/eda_gen/features/mi_rank_all_tasks.csv")
    print("üìÅ –°–æ—Ö—Ä–∞–Ω—ë–Ω –æ–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: data/eda_gen/features/mi_rank_all_tasks.csv")

    # === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è top-30 –ø–æ MI_avg ===
    top_features = mi_ranked["MI_avg"].head(30)
    plt.figure(figsize=(10, 6))
    sns.barplot(x=top_features.values, y=top_features.index, palette="mako")
    plt.title("Top-30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Å—Ä–µ–¥–Ω–µ–π MI (–≤—Å–µ –∑–∞–¥–∞—á–∏)")
    plt.xlabel("–°—Ä–µ–¥–Ω—è—è Mutual Information")
    plt.ylabel("–ü—Ä–∏–∑–Ω–∞–∫–∏")
    plt.tight_layout()
    plt.savefig("plots/eda_gen/feature_importance/overall_MI_ranking.png")
    plt.close()






    print("\n=== –≠–¢–ê–ü 6: –§–∏–Ω–∞–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (r > 0.95) —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º MI ===")

    # === –≠–¢–ê–ü 6: –§–∏–Ω–∞–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (r > 0.95) —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º MI ===


    # === –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ===
    DATA_PATH = "data/eda_gen/data_clean_pruned.csv"
    FINAL_PATH = "data/eda_gen/data_final.csv"
    FEATURES_DIR = "data/eda_gen/features"
    PLOTS_DIR = "plots/eda_gen/feature_importance"

    # === –ó–∞–≥—Ä—É–∑–∫–∞ ===
    df = pd.read_csv(DATA_PATH)
    drop_cols = [
        "IC50_nM", "CC50_nM", "SI_corrected",
        "log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI",
        "IC50_gt_median", "CC50_gt_median", "SI_gt_median", "SI_gt_8"
    ]
    X = df.drop(columns=drop_cols, errors="ignore")
    print(f"üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {X.shape[1]}")

    # === –†–∞—Å—á—ë—Ç MI –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º ===
    mi_df = pd.DataFrame(index=X.columns)
    mi_df["reg_IC50"] = mutual_info_regression(X, df["log1p_IC50_nM"], random_state=42)
    mi_df["reg_CC50"] = mutual_info_regression(X, df["log1p_CC50_nM"], random_state=42)
    mi_df["reg_SI"] = mutual_info_regression(X, df["log1p_SI"], random_state=42)
    mi_df["clf_IC50"] = mutual_info_classif(X, df["IC50_gt_median"], random_state=42)
    mi_df["clf_CC50"] = mutual_info_classif(X, df["CC50_gt_median"], random_state=42)
    mi_df["clf_SI"] = mutual_info_classif(X, df["SI_gt_median"], random_state=42)
    mi_df["clf_SI_gt_8"] = mutual_info_classif(X, df["SI_gt_8"], random_state=42)
    mi_df["MI_avg"] = mi_df.mean(axis=1)
    mi_df.to_csv(f"{FEATURES_DIR}/mi_reg_avg.csv")
    print("‚úÖ MI —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: mi_reg_avg.csv")

    # === –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π ===
    corr_matrix = X.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    plt.figure(figsize=(14, 12))
    sns.heatmap(corr_matrix, cmap="coolwarm", square=True, linewidths=0.5)
    plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–¥–æ —É–¥–∞–ª–µ–Ω–∏—è)")
    plt.tight_layout()
    plt.savefig(f"{PLOTS_DIR}/correlation_matrix.png")
    plt.close()

    # === –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–æ MI ===
    high_corr_pairs = [
        (col, row, upper_tri.loc[row, col])
        for col in upper_tri.columns
        for row in upper_tri.index
        if pd.notnull(upper_tri.loc[row, col]) and upper_tri.loc[row, col] > 0.95
    ]
    print(f"üîó –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä —Å r > 0.95: {len(high_corr_pairs)}")

    features_to_drop_final = set()
    for a, b, r in high_corr_pairs:
        if a in features_to_drop_final or b in features_to_drop_final:
            continue
        if mi_df.loc[a, "MI_avg"] >= mi_df.loc[b, "MI_avg"]:
            features_to_drop_final.add(b)
        else:
            features_to_drop_final.add(a)
    print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º: {len(features_to_drop_final)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    with open(f"{FEATURES_DIR}/high_corr_removed_by_MI.txt", "w") as f:
        for feat in sorted(features_to_drop_final):
            f.write(f"{feat}\n")

    # === –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
    df_final = df.drop(columns=features_to_drop_final, errors="ignore")
    df_final.to_csv(FINAL_PATH, index=False)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {FINAL_PATH} ({df_final.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")

    # === –°–ø–∏—Å–æ–∫ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º ===
    deleted_info = []
    for a, b, r in high_corr_pairs:
        if a in features_to_drop_final:
            kept, dropped = b, a
        elif b in features_to_drop_final:
            kept, dropped = a, b
        else:
            continue
        deleted_info.append({
            "dropped_feature": dropped,
            "kept_feature": kept,
            "correlation": r,
            "MI_dropped": mi_df.loc[dropped, "MI_avg"],
            "MI_kept": mi_df.loc[kept, "MI_avg"],
            "reason": f"r = {r:.3f}, {dropped} < {kept} –ø–æ MI"
        })
    deleted_df = pd.DataFrame(deleted_info)
    deleted_df.to_csv(f"{FEATURES_DIR}/deleted_features_with_reasons.csv", index=False)

    # === –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ ===
    corr_matrix_final = df_final.drop(columns=drop_cols, errors="ignore").corr().abs()
    plt.figure(figsize=(14, 12))
    sns.heatmap(corr_matrix_final, cmap="coolwarm", square=True, linewidths=0.5)
    plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è)")
    plt.tight_layout()
    plt.savefig(f"{PLOTS_DIR}/correlation_matrix_after.png")
    plt.close()

    # === –ü–µ—á–∞—Ç—å –≤—Å–µ—Ö —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    print("\nüìã –£–¥–∞–ª—ë–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ MI –ø—Ä–∏ r > 0.95:")
    for feat in sorted(features_to_drop_final):
        print(f"‚Ä¢ {feat}")






    # === –≠–¢–ê–ü 7: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    print("=== –≠–¢–ê–ü 7: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")

    # === –ó–∞–≥—Ä—É–∑–∫–∞ ===
    df = pd.read_csv("data/eda_gen/data_final.csv")

    # === –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º) ===
    target_cols = [
        "log1p_IC50_nM", "log1p_CC50_nM", "log1p_SI",
        "IC50_gt_median", "CC50_gt_median", "SI_gt_median", "SI_gt_8"
    ]

    # === –¢–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ ===
    X = df.drop(columns=target_cols, errors="ignore")
    X_numeric = X.select_dtypes(include=[np.number])

    # === –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–∏–Ω–∞—Ä–Ω—ã–µ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ===
    binary_features = [col for col in X_numeric.columns if set(df[col].dropna().unique()).issubset({0, 1})]
    continuous_features = [col for col in X_numeric.columns if col not in binary_features]

    print(f"üî¢ –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_numeric.shape[1]}")
    print(f"‚úÖ –ë–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(binary_features)}")
    print(f"üîß –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(continuous_features)}")

    # === –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ===
    scaler = StandardScaler()
    X_scaled = X_numeric.copy()
    X_scaled[continuous_features] = scaler.fit_transform(X_scaled[continuous_features])

    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
    X_scaled.to_csv("data/eda_gen/scaled/X_scaled.csv", index=False)

    df_scaled = pd.concat([X_scaled, df[target_cols]], axis=1)
    df_scaled.to_csv("data/eda_gen/scaled/data_scaled.csv", index=False)

    joblib.dump(scaler, "data/eda_gen/scaled/scaler_reg.pkl")

    print("üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:")
    print("‚Üí data/eda_gen/scaled/X_scaled.csv")
    print("‚Üí data/eda_gen/scaled/data_scaled.csv")
    print("‚Üí data/eda_gen/scaled/scaler_reg.pkl")


    desc = X_scaled.describe().T
    suspects = desc[desc["max"] > 500].index.tolist()
    print(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ max > 500: {suspects}")







    # === –≠–¢–ê–ü 8: –°–≤–æ–¥–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (PCA, UMAP, LDA) –ø–æ –∑–∞–¥–∞—á–∞–º ===
    print("=== –≠–¢–ê–ü 8: –°–≤–æ–¥–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (PCA, UMAP, LDA) –ø–æ –∑–∞–¥–∞—á–∞–º ===")

    # === –î–∞–Ω–Ω—ã–µ ===
    X_scaled = pd.read_csv("data/eda_gen/scaled/X_scaled.csv")
    df = pd.read_csv("data/eda_gen/data_final.csv")

    tasks = {
        "reg_log1p_IC50_nM": "log1p_IC50_nM",
        "reg_log1p_CC50_nM": "log1p_CC50_nM",
        "reg_log1p_SI": "log1p_SI",
        "clf_IC50_gt_median": "IC50_gt_median",
        "clf_CC50_gt_median": "CC50_gt_median",
        "clf_SI_gt_median": "SI_gt_median",
        "clf_SI_gt_8": "SI_gt_8"
    }

    # === –ì–ª–æ–±–∞–ª—å–Ω–∞—è PCA –ø–æ –≤—Å–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º ===
    print("üìä –ì–ª–æ–±–∞–ª—å–Ω–∞—è PCA –Ω–∞ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö X_scaled")
    pca_all = PCA(n_components=None, random_state=42)
    X_pca_all = pca_all.fit_transform(X_scaled)
    explained_var = pca_all.explained_variance_ratio_
    cumulative_var = np.cumsum(explained_var)

    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(explained_var)+1), explained_var, marker='o', label="–û–±—ä—è—Å–Ω—ë–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è")
    plt.plot(range(1, len(cumulative_var)+1), cumulative_var, marker='s', label="–ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è")
    plt.axhline(0.95, color='red', linestyle='--', label="95% –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
    plt.xlabel("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞")
    plt.ylabel("–î–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
    plt.title("PCA: –æ–±—ä—è—Å–Ω—ë–Ω–Ω–∞—è –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è (–≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    os.makedirs("plots/eda_gen/projections_variance", exist_ok=True)
    plt.savefig("plots/eda_gen/projections_variance/pca_explained_variance_ALL.png")
    plt.show()

    # === PCA –ø–æ –∑–∞–¥–∞—á–∞–º ===
    pca_components_95 = []

    print("\nüìä PCA –ø–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏:")
    for task, target_col in tasks.items():
        feat_path = f"data/eda_gen/features/{task}.txt"
        with open(feat_path) as f:
            features_raw = [line.strip() for line in f]
            features = [col for col in features_raw if col in X_scaled.columns]

        print(f"üìÅ {task}: {len(features_raw)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ .txt ‚Üí {len(features)} –∑–∞–≥—Ä—É–∂–µ–Ω–æ")

        X_task = X_scaled[features]
        pca = PCA(n_components=None, random_state=42)
        pca.fit(X_task)
        explained_var = pca.explained_variance_ratio_
        cumulative_var = np.cumsum(explained_var)
        n_components_95 = np.argmax(cumulative_var >= 0.95) + 1

        print(f"‚úÖ {task}: {n_components_95} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è ‚â•95% –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∏–∑ {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        pca_components_95.append({
            "task": task,
            "n_features_loaded": len(features),
            "n_components_95": n_components_95
        })

    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ PCA
    df_pca_summary = pd.DataFrame(pca_components_95)
    os.makedirs("data/eda_gen/features", exist_ok=True)
    df_pca_summary.to_csv("data/eda_gen/features/pca_n_components_95.csv", index=False)
    print("\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: data/eda_gen/features/pca_n_components_95.csv")

    # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ö–æ–ª—Å—Ç–∞ –ø–æ–¥ –≥—Ä–∞—Ñ–∏–∫–∏ (PCA / UMAP / LDA)
    import umap
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

    n_rows = len(tasks)
    n_cols = 3
    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))
    fig.subplots_adjust(hspace=0.4, wspace=0.3)

    for row_idx, (task, target_col) in enumerate(tasks.items()):
        feat_path = f"data/eda_gen/features/{task}.txt"
        if not os.path.exists(feat_path):
            print(f"‚ùå –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {task}")
            continue

        with open(feat_path) as f:
            raw_features = [line.strip() for line in f]
        features = [f for f in raw_features if f in X_scaled.columns]

        print(f"üìÅ {task}: {len(raw_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ .txt ‚Üí {len(features)} –∑–∞–≥—Ä—É–∂–µ–Ω–æ")

        if len(features) < 2:
            print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {task}")
            continue

        X_task = X_scaled[features]
        y = df[target_col]
        is_clf = task.startswith("clf_")

        # PCA
        ax = axs[row_idx, 0]
        X_pca = PCA(n_components=2, random_state=42).fit_transform(X_task)
        ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap="tab10" if is_clf else "viridis", alpha=0.7)
        ax.set_title(f"{task}\nPCA")
        ax.set_xlabel("PC1")
        ax.set_ylabel("PC2")

        # UMAP
        ax = axs[row_idx, 1]
        X_umap = umap.UMAP(n_components=2, random_state=42).fit_transform(X_task)
        ax.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap="tab10" if is_clf else "plasma", alpha=0.7)
        ax.set_title(f"{task}\nUMAP")
        ax.set_xlabel("UMAP1")
        ax.set_ylabel("UMAP2")

        # LDA
        ax = axs[row_idx, 2]
        if is_clf:
            y_array = y.values
            n_classes = len(np.unique(y_array))
            max_components = min(len(features), n_classes - 1)
            if max_components >= 1:
                lda = LinearDiscriminantAnalysis(n_components=max_components)
                X_lda = lda.fit_transform(X_task, y_array)
                if max_components == 1:
                    ax.scatter(X_lda[:, 0], [0]*len(X_lda), c=y_array, cmap="coolwarm", alpha=0.7)
                    ax.set_yticks([])
                else:
                    ax.scatter(X_lda[:, 0], X_lda[:, 1], c=y_array, cmap="coolwarm", alpha=0.7)
                    ax.set_ylabel("LDA2")
                ax.set_xlabel("LDA1")
                ax.set_title(f"{task}\nLDA")
            else:
                ax.axis("off")
        else:
            ax.axis("off")

    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
    os.makedirs("plots/eda_gen/projections_task", exist_ok=True)
    fig.tight_layout()
    fig.savefig("plots/eda_gen/projections_task/_ALL_TASKS.png", dpi=300)
    plt.show()
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: _ALL_TASKS.png –∏ –æ—Ç–æ–±—Ä–∞–∂—ë–Ω –Ω–∞ —ç–∫—Ä–∞–Ω–µ.")






    # === –≠–¢–ê–ü 9: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ EDA –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ ===

    print("\n=== –≠–¢–ê–ü 9: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ EDA ===")

    # === –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ===
    df_final = pd.read_csv("data/eda_gen/data_final.csv")
    X_scaled = pd.read_csv("data/eda_gen/scaled/X_scaled.csv")

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ ===
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º: {df_final.shape[0]} —Å—Ç—Ä–æ–∫, {df_final.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
    print(f"‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {X_scaled.shape[0]} —Å—Ç—Ä–æ–∫, {X_scaled.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    # === –°–ø–∏—Å–æ–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
    final_features = X_scaled.columns.tolist()
    with open("data/eda_gen/features/final_feature_list.txt", "w") as f:
        for feat in final_features:
            f.write(f"{feat}\n")
    print("üìù –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: data/eda_gen/features/final_feature_list.txt")

    # === –õ–æ–≥ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ===
    print("\nüì¶ –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(" ‚Ä¢ data/eda_gen/data_clean.csv ‚Äî –ø–æ—Å–ª–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ outlier-–æ—á–∏—Å—Ç–∫–∏")
    print(" ‚Ä¢ data/eda_gen/data_clean_pruned.csv ‚Äî –ø–æ—Å–ª–µ Feature Engineering –∏ —É–¥–∞–ª–µ–Ω–∏—è –ø–ª–æ—Ö–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(" ‚Ä¢ data/eda_gen/data_final.csv ‚Äî –ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
    print(" ‚Ä¢ data/eda_gen/scaled/X_scaled.csv ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    print(" ‚Ä¢ data/eda_gen/features/final_feature_list.txt ‚Äî –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")

    print("\n‚úÖ –≠—Ç–∞–ø EDA –∑–∞–≤–µ—Ä—à—ë–Ω.")




if __name__ == "__main__":
    main()
